# Guia: Criando uma Stack Warmly com o Stack Manager

Este guia fornece instruÃ§Ãµes detalhadas sobre como criar e implantar uma nova stack Warmly-AI para um cliente usando o Stack Manager.

## ğŸ“‹ Ãndice

- [PrÃ©-requisitos](#prÃ©-requisitos)
- [VisÃ£o Geral](#visÃ£o-geral)
- [Passo 1: Preparar InformaÃ§Ãµes do Cliente](#passo-1-preparar-informaÃ§Ãµes-do-cliente)
- [Passo 2: Usar o Stack Manager](#passo-2-usar-o-stack-manager)
- [Passo 3: Configurar Credenciais](#passo-3-configurar-credenciais)
- [Passo 4: Personalizar Prompts](#passo-4-personalizar-prompts)
- [Passo 5: Popular o Banco Vetorial](#passo-5-popular-o-banco-vetorial)
- [Passo 6: Deploy da Stack](#passo-6-deploy-da-stack)
- [Passo 7: Configurar Webhook do WAHA](#passo-7-configurar-webhook-do-waha)
- [VerificaÃ§Ã£o e Testes](#verificaÃ§Ã£o-e-testes)
- [Troubleshooting](#troubleshooting)

## ğŸ”§ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter:

- âœ… Acesso ao servidor de infraestrutura
- âœ… Docker e Docker Compose instalados e funcionando
- âœ… Stack Manager clonado e atualizado (`git submodule update --init --recursive`)
- âœ… Credenciais do Google Cloud (Service Account) com acesso ao BigQuery
- âœ… InformaÃ§Ãµes do cliente (nome, domÃ­nio, ID do WAHA)
- âœ… Documentos/dados para o banco vetorial (opcional, mas recomendado)
- âœ… Chave API do provedor de LLM (OpenAI, Anthropic, etc.) ou Ollama rodando localmente

## ğŸ¯ VisÃ£o Geral

O processo de criaÃ§Ã£o de uma stack Warmly envolve:

1. **PreparaÃ§Ã£o**: Coletar informaÃ§Ãµes do cliente
2. **GeraÃ§Ã£o**: Usar o Stack Manager para criar a estrutura
3. **ConfiguraÃ§Ã£o**: Definir variÃ¡veis de ambiente e credenciais
4. **PersonalizaÃ§Ã£o**: Customizar prompts e comportamento do agente
5. **Deploy**: Implantar a stack e inicializar serviÃ§os
6. **IntegraÃ§Ã£o**: Conectar com WAHA (WhatsApp) e testar

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Stack Manager                             â”‚
â”‚                         â†“                                    â”‚
â”‚              Gera estrutura do cliente                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   clients/{cliente}/warmly-ai/                               â”‚
â”‚   â”œâ”€â”€ docker-compose.yml                                    â”‚
â”‚   â”œâ”€â”€ .env                                                  â”‚
â”‚   â”œâ”€â”€ data/                                                 â”‚
â”‚   â”‚   â””â”€â”€ service-account.google.json                       â”‚
â”‚   â””â”€â”€ prompts/                                              â”‚
â”‚       â”œâ”€â”€ system.md                                         â”‚
â”‚       â”œâ”€â”€ evaluate_tools.md                                 â”‚
â”‚       â””â”€â”€ ...                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    Deploy e InicializaÃ§Ã£o
```

## ğŸ“ Passo 1: Preparar InformaÃ§Ãµes do Cliente

Colete as seguintes informaÃ§Ãµes antes de comeÃ§ar:

### InformaÃ§Ãµes ObrigatÃ³rias

| Campo | DescriÃ§Ã£o | Exemplo |
|-------|-----------|---------|
| **Nome do Cliente** | Identificador Ãºnico do cliente | `acme-corp` |
| **DomÃ­nio** | SubdomÃ­nio para acesso via Traefik | `acme.lvh.me` ou `acme.seudominio.com` |
| **ID do Cliente WAHA** | ID usado pelo WAHA para identificar este cliente | `acme-123` ou `5511999999999` |
| **BigQuery Table ID** | Tabela para registro de intenÃ§Ãµes | `seu-projeto.warmly.purchase_intents` |
| **LLM Provider** | Provedor do modelo de linguagem | `openai`, `anthropic`, `gemini`, `ollama` |
| **LLM Model** | Modelo especÃ­fico | `gpt-4o`, `claude-3-5-sonnet-20241022`, `mistral` |

### InformaÃ§Ãµes Opcionais

- **Documentos para RAG**: PDFs, TXTs, DOCXs com informaÃ§Ãµes do cliente
- **Porta customizada**: Se nÃ£o quiser usar portas automÃ¡ticas
- **ConfiguraÃ§Ãµes especÃ­ficas**: Temperatura do LLM, top_k, etc.

### Template de Coleta

Crie um arquivo `cliente-info.txt`:

```txt
# InformaÃ§Ãµes do Cliente: ACME Corp
# Data: 2025-10-03

## IdentificaÃ§Ã£o
CLIENTE_ID=acme-corp
CLIENTE_NOME="ACME Corporation"
DOMINIO=acme.lvh.me

## WAHA Integration
WAHA_CLIENT_ID=5511999999999
WAHA_WEBHOOK_URL=http://warmly-ai-acme:8000/api/waha/webhook

## BigQuery
BIGQUERY_PROJECT_ID=warmly-production
BIGQUERY_DATASET=clients_data
BIGQUERY_TABLE_ID=warmly-production.clients_data.purchase_intents

## LLM Configuration
LLM_PROVIDER=openai
LLM_MODEL_NAME=gpt-4o
LLM_API_KEY=sk-proj-...
LLM_TEMPERATURE=0

## Network Configuration
REDE_EDGE=edge
REDE_SHARED=shared
PORTA_BACKEND=8000
PORTA_FRONTEND=8501
```

## ğŸš€ Passo 2: Usar o Stack Manager

### OpÃ§Ã£o A: Comando CLI do Stack Manager

```bash
cd /home/rfluid/development/Warmly/infra/stack-manager

# Criar nova stack para o cliente
./stack-manager create \
  --client-name acme-corp \
  --domain acme.lvh.me \
  --waha-id 5511999999999 \
  --bigquery-table warmly-production.clients_data.purchase_intents \
  --llm-provider openai \
  --llm-model gpt-4o

# Output esperado:
# âœ… Stack criada em: /home/rfluid/development/Warmly/infra/clients/acme-corp/warmly-ai
# â„¹ï¸  PrÃ³ximos passos:
#    1. Configurar .env com suas credenciais
#    2. Adicionar service-account.google.json em data/
#    3. Personalizar prompts/ conforme necessÃ¡rio
#    4. Executar: ./scripts/init_all.sh
```

### OpÃ§Ã£o B: CriaÃ§Ã£o Manual (se Stack Manager nÃ£o estiver disponÃ­vel)

Se o stack manager ainda nÃ£o estiver completamente implementado, vocÃª pode criar manualmente:

```bash
cd /home/rfluid/development/Warmly/infra

# 1. Criar estrutura de diretÃ³rios
mkdir -p clients/acme-corp/warmly-ai/{data,prompts}

# 2. Copiar template do warmly-ai
cp -r warmly-ai/docker-compose.yml clients/acme-corp/warmly-ai/
cp warmly-ai/example.env clients/acme-corp/warmly-ai/.env
cp -r warmly-ai/prompts/* clients/acme-corp/warmly-ai/prompts/

# 3. Renomear arquivos de exemplo
cd clients/acme-corp/warmly-ai/prompts
for file in *.example.md; do
  mv "$file" "${file%.example.md}.md"
done
```

### Estrutura Gerada

ApÃ³s executar o Stack Manager, vocÃª terÃ¡:

```
clients/acme-corp/warmly-ai/
â”œâ”€â”€ docker-compose.yml          # ConfiguraÃ§Ã£o dos serviÃ§os
â”œâ”€â”€ .env                        # VariÃ¡veis de ambiente
â”œâ”€â”€ data/                       # Dados persistentes
â”‚   â”œâ”€â”€ service-account.google.json  # Credenciais GCP (adicionar)
â”‚   â””â”€â”€ documents/              # Documentos para RAG (opcional)
â”œâ”€â”€ prompts/                    # Prompts customizÃ¡veis
â”‚   â”œâ”€â”€ system.md              # Prompt de sistema (personalidade)
â”‚   â”œâ”€â”€ evaluate_tools.md      # LÃ³gica de seleÃ§Ã£o de ferramentas
â”‚   â”œâ”€â”€ evaluate_tools_parallel.md
â”‚   â”œâ”€â”€ error_handler.md       # Tratamento de erros
â”‚   â”œâ”€â”€ summarize.md           # SumarizaÃ§Ã£o de conversas
â”‚   â””â”€â”€ user.md                # Template de mensagens do usuÃ¡rio
â””â”€â”€ README.md                  # DocumentaÃ§Ã£o especÃ­fica do cliente
```

## ğŸ” Passo 3: Configurar Credenciais

### 3.1. Editar `.env`

Navegue atÃ© o diretÃ³rio da stack e edite o arquivo `.env`:

```bash
cd clients/acme-corp/warmly-ai
nano .env  # ou vim, code, etc.
```

Configure as seguintes variÃ¡veis **essenciais**:

```bash
# ============================================
# CONFIGURAÃ‡ÃƒO DO CLIENTE: ACME Corp
# ============================================

# Ambiente
ENV=production

# LLM Configuration
LLM_PROVIDER=openai
LLM_MODEL_NAME=gpt-4o
LLM_API_KEY=sk-proj-SUAS_CREDENCIAIS_AQUI
LLM_TEMPERATURE=0

# Tool Evaluator (pode usar o mesmo ou modelo mais barato)
TOOL_EVALUATOR_LLM_PROVIDER=openai
TOOL_EVALUATOR_LLM_MODEL_NAME=gpt-4o-mini
TOOL_EVALUATOR_LLM_API_KEY=sk-proj-SUAS_CREDENCIAIS_AQUI
TOOL_EVALUATOR_LLM_TEMPERATURE=0

# Summarizer (pode usar modelo mais barato)
SUMMARIZE_LLM_PROVIDER=openai
SUMMARIZE_LLM_MODEL_NAME=gpt-4o-mini
SUMMARIZE_LLM_API_KEY=sk-proj-SUAS_CREDENCIAIS_AQUI
SUMMARIZE_LLM_TEMPERATURE=0

# Text Embedding
TEXT_EMBEDDING_PROVIDER=openai
TEXT_EMBEDDING_MODEL_NAME=text-embedding-3-small
TEXT_EMBEDDING_API_KEY=sk-proj-SUAS_CREDENCIAIS_AQUI

# Paths (dentro do container)
DATA_DIR=/app/data
PROMPTS_DIR=/app/prompts

# API URL (interno)
API_URL=http://localhost:8000

# Database (usa PostgreSQL compartilhado)
POSTGRES_URI=postgresql://postgres:postgres@database:5432/postgres?sslmode=disable

# Vector Database (usa Milvus compartilhado)
MILVUS_URI=http://milvus:19530
MILVUS_COLLECTION=acme_corp_collection
RAG_AVAILABLE=true

# BigQuery
GOOGLE_APPLICATION_CREDENTIALS=/app/data/service-account.google.json
BIGQUERY_TABLE_ID=warmly-production.clients_data.purchase_intents

# Features
PARALLEL_GENERATION=false
```

### 3.2. Adicionar Service Account do Google Cloud

```bash
# Copiar arquivo de credenciais para o diretÃ³rio data/
cp ~/Downloads/warmly-production-sa-key.json \
   clients/acme-corp/warmly-ai/data/service-account.google.json

# Ajustar permissÃµes
chmod 600 clients/acme-corp/warmly-ai/data/service-account.google.json
```

**Importante**: A Service Account precisa ter a role **"BigQuery Data Editor"** no projeto.

### 3.3. Configurar docker-compose.yml

Edite o `docker-compose.yml` para incluir labels do Traefik e configuraÃ§Ãµes de rede:

```yaml
services:
  warmly-ai:
    image: warmly-ai:latest
    container_name: warmly-ai-acme
    restart: unless-stopped
    env_file:
      - .env
    environment:
      POSTGRES_URI: postgresql://postgres:postgres@database:5432/postgres?sslmode=disable
      MILVUS_URI: http://milvus:19530
      DATA_DIR: /app/data
      PROMPTS_DIR: /app/prompts
      API_URL: http://localhost:8000
    volumes:
      - ./data:/app/data
      - ./prompts:/app/prompts
    networks:
      - edge    # Para acesso via Traefik
      - shared  # Para PostgreSQL e Milvus
    labels:
      # Backend API
      - "traefik.enable=true"
      - "traefik.docker.network=edge"
      - "traefik.http.routers.acme-api.rule=Host(`api.acme.lvh.me`)"
      - "traefik.http.routers.acme-api.entrypoints=web"
      - "traefik.http.services.acme-api.loadbalancer.server.port=8000"
      
      # Frontend (opcional, se quiser expor)
      - "traefik.http.routers.acme-frontend.rule=Host(`chat.acme.lvh.me`)"
      - "traefik.http.routers.acme-frontend.entrypoints=web"
      - "traefik.http.services.acme-frontend.loadbalancer.server.port=8501"

networks:
  edge:
    external: true
  shared:
    external: true
```

## ğŸ¨ Passo 4: Personalizar Prompts

Veja o guia detalhado em [02-edit-prompts.md](./02-edit-prompts.md).

Resumo rÃ¡pido:

```bash
cd clients/acme-corp/warmly-ai/prompts

# Editar personalidade do agente
nano system.md

# Editar lÃ³gica de seleÃ§Ã£o de ferramentas
nano evaluate_tools.md
```

## ğŸ“š Passo 5: Popular o Banco Vetorial

### 5.1. Preparar Documentos

```bash
cd clients/acme-corp/warmly-ai/data
mkdir -p documents

# Copiar documentos do cliente (PDFs, TXTs, etc.)
cp /path/to/client/docs/* documents/
```

### 5.2. Upload via API

Existem duas formas de popular o banco vetorial:

#### OpÃ§Ã£o A: Via Frontend Streamlit

1. Acesse `http://chat.acme.lvh.me` (se exposto)
2. Use a interface de upload de documentos
3. Aguarde o processamento

#### OpÃ§Ã£o B: Via API REST

```bash
# Fazer upload de documentos via API
curl -X POST "http://api.acme.lvh.me/api/vectorstore/upload" \
  -F "files=@documents/catalogo-produtos.pdf" \
  -F "files=@documents/politicas-vendas.txt"

# Response esperado:
# {
#   "message": "2 documents uploaded successfully",
#   "collection": "acme_corp_collection"
# }
```

#### OpÃ§Ã£o C: Script Python

```python
#!/usr/bin/env python3
# upload_docs.py
import requests
from pathlib import Path

API_URL = "http://api.acme.lvh.me"
DOCS_DIR = Path("./data/documents")

files = []
for doc in DOCS_DIR.glob("*"):
    if doc.is_file():
        files.append(("files", (doc.name, open(doc, "rb"))))

response = requests.post(
    f"{API_URL}/api/vectorstore/upload",
    files=files
)

print(response.json())
```

Executar:

```bash
python3 upload_docs.py
```

## ğŸš€ Passo 6: Deploy da Stack

### 6.1. Build da Imagem Warmly-AI (se necessÃ¡rio)

Se ainda nÃ£o tiver a imagem `warmly-ai:latest`:

```bash
cd /home/rfluid/development/Warmly/infra/warmly-ai

# Build da imagem
make build

# Ou manualmente:
docker build -t warmly-ai:latest .

# Verificar
docker images | grep warmly-ai
```

### 6.2. Inicializar Todas as Stacks

```bash
cd /home/rfluid/development/Warmly/infra

# Inicializar tudo (incluindo a nova stack)
./scripts/init_all.sh

# Ou apenas a stack do cliente
cd clients/acme-corp/warmly-ai
docker compose up -d
```

### 6.3. Verificar Logs

```bash
# Ver logs da stack
cd clients/acme-corp/warmly-ai
docker compose logs -f

# Verificar se estÃ¡ saudÃ¡vel
docker compose ps
```

Output esperado:

```
NAME                IMAGE              STATUS
warmly-ai-acme      warmly-ai:latest   Up 2 minutes (healthy)
```

## ğŸ“² Passo 7: Configurar Webhook do WAHA

### 7.1. Obter InformaÃ§Ãµes do WAHA

VocÃª precisarÃ¡:
- **WAHA_URL**: URL da instÃ¢ncia WAHA (ex: `http://waha:3000`)
- **WAHA_API_KEY**: Chave API do WAHA
- **SESSION_ID**: ID da sessÃ£o do WhatsApp (geralmente o nÃºmero)

### 7.2. Configurar Webhook

Edite e execute o script:

```bash
cd /home/rfluid/development/Warmly/infra/scripts

# Configurar variÃ¡veis
export WAHA_API_KEY_PLAIN="sua-chave-api-waha"
export WEBHOOK_URL="http://warmly-ai-acme:8000/api/waha/webhook"

# Executar script
./init_waha_webhook.sh

# Output esperado:
# â³ Waiting for Waha to become available...
# âœ… Waha is up. Configuring webhook for session 'default'...
# âœ… Webhook registered successfully.
```

### 7.3. Testar IntegraÃ§Ã£o WAHA â†’ Warmly-AI

Envie uma mensagem via WhatsApp para o nÃºmero configurado:

```
OlÃ¡! Quero saber sobre os produtos.
```

Verifique os logs:

```bash
cd clients/acme-corp/warmly-ai
docker compose logs -f warmly-ai

# VocÃª deve ver:
# [INFO] Received webhook from WAHA: customer_id=5511999999999
# [INFO] Processing message: "OlÃ¡! Quero saber sobre os produtos."
# [INFO] Using RAG tool to search knowledge base...
# [INFO] Sending response via WAHA...
```

## âœ… VerificaÃ§Ã£o e Testes

### 1. Health Check da API

```bash
curl http://api.acme.lvh.me/health

# Response esperado:
# {"status": "healthy", "version": "1.0.0"}
```

### 2. Teste de Conversa via API

```bash
curl -X POST "http://api.acme.lvh.me/api/messages/send" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "OlÃ¡, quais produtos vocÃªs oferecem?",
    "thread_id": "test-123",
    "customer_id": "test-customer"
  }'
```

### 3. Verificar BigQuery

Envie uma mensagem que gere intenÃ§Ã£o de compra e verifique no BigQuery:

```sql
SELECT *
FROM `warmly-production.clients_data.purchase_intents`
WHERE customer_id = '5511999999999'
ORDER BY event_timestamp DESC
LIMIT 10;
```

### 4. Teste End-to-End via WhatsApp

1. **Enviar mensagem inicial**: "OlÃ¡!"
2. **Perguntar sobre produto**: "Quero saber sobre o produto X"
3. **Expressar intenÃ§Ã£o**: "Ok, vou comprar o produto X"
4. **Verificar confirmaÃ§Ã£o**: Deve receber confirmaÃ§Ã£o via WhatsApp
5. **Verificar BigQuery**: Deve ter registro da intenÃ§Ã£o

### 5. Monitorar via Dashboards

- **Dashy**: `http://dashboard.lvh.me` - Adicionar links para a nova stack
- **Portainer**: `http://portainer.lvh.me` - Ver containers e logs
- **Traefik**: `http://traefik.lvh.me` - Ver roteadores configurados

## ğŸ› Troubleshooting

### Problema: Container nÃ£o inicia

```bash
# Ver logs detalhados
docker compose logs warmly-ai

# Verificar configuraÃ§Ã£o
docker compose config

# Recriar container
docker compose up -d --force-recreate warmly-ai
```

### Problema: NÃ£o conecta ao PostgreSQL

```bash
# Verificar se database estÃ¡ rodando
docker ps | grep database

# Testar conexÃ£o manualmente
docker exec warmly-ai-acme psql postgresql://postgres:postgres@database:5432/postgres -c "SELECT 1"
```

### Problema: NÃ£o conecta ao Milvus

```bash
# Verificar se Milvus estÃ¡ rodando
docker ps | grep milvus

# Testar conexÃ£o
docker exec warmly-ai-acme curl http://milvus:19530/healthz
```

### Problema: BigQuery - Permission Denied

```bash
# Verificar se service account tem permissÃµes
gcloud projects get-iam-policy warmly-production \
  --flatten="bindings[].members" \
  --filter="bindings.members:serviceAccount:*warmly*"

# Adicionar role se necessÃ¡rio
gcloud projects add-iam-policy-binding warmly-production \
  --member="serviceAccount:warmly-sa@warmly-production.iam.gserviceaccount.com" \
  --role="roles/bigquery.dataEditor"
```

### Problema: Webhook WAHA nÃ£o funciona

```bash
# Verificar se WAHA alcanÃ§a o container
docker exec waha curl http://warmly-ai-acme:8000/health

# Verificar logs do WAHA
docker logs waha -f

# Reconfigurar webhook
export WAHA_API_KEY_PLAIN="..."
export WEBHOOK_URL="http://warmly-ai-acme:8000/api/waha/webhook"
./scripts/init_waha_webhook.sh
```

### Problema: LLM nÃ£o responde

```bash
# Verificar variÃ¡veis de ambiente
docker exec warmly-ai-acme env | grep LLM

# Testar API key
docker exec warmly-ai-acme python3 -c "
import os
from openai import OpenAI
client = OpenAI(api_key=os.getenv('LLM_API_KEY'))
print(client.models.list())
"
```

## ğŸ“Š PrÃ³ximos Passos

ApÃ³s criar e implantar a stack:

1. âœ… **Adicionar ao Dashboard**: Edite `platform/dashy.conf.yml` para incluir links
2. âœ… **Configurar Monitoramento**: Adicione health checks em `platform/monitors/`
3. âœ… **Documentar CustomizaÃ§Ãµes**: Crie um README especÃ­fico do cliente
4. âœ… **Treinar a IA**: Adicione mais documentos ao banco vetorial
5. âœ… **Refinar Prompts**: Ajuste o comportamento baseado em feedback
6. âœ… **Backup**: Configure backup dos volumes de dados

## ğŸ“š ReferÃªncias

- [Guia de EdiÃ§Ã£o de Prompts](./02-edit-prompts.md)
- [IDs de Cliente WAHA â†” BigQuery](./03-client-ids-waha-bigquery.md)
- [Warmly-AI README](../warmly-ai/README.md)
- [README Principal](../README.md)

## ğŸ†˜ Suporte

Em caso de problemas:
1. Consulte a seÃ§Ã£o de Troubleshooting
2. Verifique os logs: `docker compose logs -f`
3. Abra uma issue no repositÃ³rio
4. Contate a equipe de infraestrutura

